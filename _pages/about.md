---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- ## About Me -->
I am a third-year Ph.D. candidate at Fudan University, advised by Prof. [Xipeng Qiu](https://xpqiu.github.io/). 
Currently, I am also interning at [Shanghai AI Laboratory](https://www.shlab.org.cn/).

My research interests lie in the field of Machine Learning and Natural Language Processing, with a particular focus on large language models and methods to enhance their efficiency and efficacy. 

# News
- [May 2024] [LOMO](https://arxiv.org/abs/2306.09782) and [AdaLomo](https://arxiv.org/abs/2310.10195) were integrated in [`transformers`](https://huggingface.co/docs/transformers/main/en/trainer#lomo-optimizer) and [`accelerate`](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator.lomo_backward)!
- [Jan. 2024] [InternLM2](https://github.com/InternLM/InternLM) with perfect performance on 200K ``Needle-in-a-Haystack" was released! 

# Education
- **Fudan University**  
  Ph.D. candidate in Computer Science, 2021 - 2026 (expected)  
  Advisor: Prof. Xipeng Qiu  
- **Fudan University**  
  B.S. in Computer Science, 2017 - 2021  

# Experience
- **Shanghai AI Laboratory**  
  Advisor: Dr. [Hang Yan](https://scholar.google.com/citations?user=yigHzW8AAAAJ&hl=zh-CN&oi=ao)  
  July 2023 - Present

# Publications
\* denotes co-first authors
<!-- $^\dagger$ denotes corresponding author/main advisor -->

**InternLM2 Technical Report**  
InternLM Team  
Tenical Report. [[paper]](https://arxiv.org/abs/2403.17297) [[github]](https://github.com/InternLM/InternLM)

**LongWanjuan: Towards Systematic Measurement for Long Text Quality**  
**Kai Lv**\*, Xiaoran Liu\*, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin  
arXiv 2024. [[paper]](https://arxiv.org/abs/2402.13583) [[code]](https://github.com/OpenLMLab/LongWanjuan)

**AdaLomo: Low-memory Optimization with Adaptive Learning Rate**  
**Kai Lv**, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu  
ACL 2024 Findings. [[paper]](https://arxiv.org/abs/2310.10195) [[code]](https://github.com/OpenLMLab/LOMO)

**CoLLiE: Collaborative Training of Large Language Models in an Efficient Way**  
**Kai Lv**\*, Shuo Zhang\*, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu  
EMNLP 2023 Demo. [[paper]](https://arxiv.org/abs/2312.00407) [[code]](https://github.com/OpenLMLab/collie)

**Full Parameter Fine-tuning for Large Language Models with Limited Resources**  
**Kai Lv**, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu  
ACL 2024. [[paper]](https://arxiv.org/abs/2306.09782) [[code]](https://github.com/OpenLMLab/LOMO)

**Unified Demonstration Retriever for In-Context Learning**  
Xiaonan Li\*, **Kai Lv**\*, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu  
ACL 2023 (oral). [[paper]](https://aclanthology.org/2023.acl-long.256/) [[code]](https://github.com/KaiLv69/UDR)

**CoNT: Contrastive Neural Text Generation**  
Chenxin An, Jiangtao Feng, **Kai Lv**, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang  
NeurIPS 2022. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0f5fcf4bff73a3537e0813a38f0d3f76-Abstract-Conference.html) [[code]](https://github.com/Shark-NLP/CoNT)

# Service
Reviewer: CoNLL
