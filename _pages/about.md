---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- ## About Me -->
I am a tenure-track assistant professor in [Shanghai Jiao Tong University](https://www.sjtu.edu.cn/), [John Hopcroft Center for Computer Science](http://jhc.sjtu.edu.cn/).
I obtained my PhD from [Carnegie Mellon University](https://www.cmu.edu), [Language Technologies Institute](https://www.lti.cs.cmu.edu) in the [School of Computer Science](https://www.cs.cmu.edu) in 2022, where I was co-advised by [Graham Neubig](http://www.phontron.com) and [Taylor Berg-Kirkpatrick](https://cseweb.ucsd.edu/~tberg/). Before that, I received the bachelor degree in Electronic Engineering from Shanghai Jiao Tong University in 2017. I also spent some time at Facebook AI Research (2019) and Salesforce Research (2020).  

<!-- I have interned at Facebook AI Research (2019), working with [Jiatao Gu](https://jiataogu.me) and [Marc'Aurelio Ranzato](https://ranzato.github.io); interned at Salesforce Research (2020), working with [Bryan McCann](https://bmccann.github.io); and visited Machine Learning Department of Carnegie Mellon University (2016), working with [Zhiting Hu](http://zhiting.ucsd.edu) and [Eric Xing](https://www.cs.cmu.edu/~epxing/).  -->

### <span style="color:red">Prospective Students:</span>
Please drop me an email with your CV if you are interested in working with me. Students at all levels (PhD/master/undergrad) are welcome. 

# Research
        
I am generally interested in natural language processing and machine learning. Current interests include: 
- Semi-parametric methods (retrieval-augmented)
- Modular approaches in NLP
- Efficient methods for large-scale models
- Data-centric NLP
- Neuro-symbolic approaches
- Learning from small data
- Controllable text generation

<!-- My research covers (latent-variable) generative models, controllable text generation, efficient text generation, and non-parametric language models.  -->


# Publications
$^\dagger$ denotes corresponding author/main advisor

**C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models**  
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, *Junxian He*$^\dagger$  
Preprint 2023. [[arxiv]](https://arxiv.org/abs/2305.08322) [[github]](https://github.com/SJTU-LIT/ceval) [[website]](https://cevalbenchmark.com)

**Mega: Moving Average Equipped Gated Attention**  
Xuezhe Ma\*, Chunting Zhou\*  (equal contribution), Xiang Kong, *Junxian He*, Liangke Gui, Graham Neubig, Jonathan May, Luke Zettlemoyer  
ICLR 2023. [[arxiv]](https://arxiv.org/abs/2209.10655)

**CTRLsum: Towards Generic Controllable Text Summarization**  
*Junxian He*, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, Caiming Xiong  
EMNLP 2022. [[arxiv]](https://arxiv.org/abs/2012.04281) [[code]](https://github.com/salesforce/ctrl-sum) [[huggingface demo]](https://huggingface.co/spaces/akhaliq/ctrl-sum) [[streamlit demo]](https://share.streamlit.io/jxhe/ctrlsum-demo/ctrlsum_demo.py)

**Prompt Consistency for Zero-Shot Task Generalization**  
Chunting Zhou* , *Junxian He*\* (equal contribution), Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig  
EMNLP 2022 Findings. [[arxiv]](https://arxiv.org/abs/2205.00049)

**Towards a Unified View of Parameter-Efficient Transfer Learning**  
*Junxian He*\*, Chunting Zhou* (equal contribution), Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig   
ICLR 2022 (<span style="color:red">spotlight, 5%</span>). [[OpenReview]](https://openreview.net/forum?id=0RDcd5Axok) [[arxiv]](http://arxiv.org/abs/2110.04366) [[code]](https://github.com/jxhe/unify-parameter-efficient-tuning)

**Capturing Structural Locality in Non-parametric Language Models**  
Frank F. Xu, *Junxian He*, Graham Neubig, Vincent Josua Hellendoorn  
ICLR 2022. [[arxiv]](https://arxiv.org/abs/2110.02870)

**Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval**  
Uri Alon, Frank F. Xu, *Junxian He*, Sudipta Sengupta, Dan Roth, Graham Neubig  
ICML 2022. [[arxiv]](https://arxiv.org/abs/2201.12431) [[code]](https://github.com/neulab/retomaton)

**Efficient Nearest Neighbor Language Models**  
*Junxian He*, Graham Neubig, Taylor Berg-Kirkpatrick  
EMNLP 2021. [[arxiv]](https://arxiv.org/abs/2109.04212) [[code]](https://github.com/jxhe/efficient-knnlm)

**The Source-Target Domain Mismatch Problem in Machine Translation**  
Jiajun Shen, Peng-Jen Chen, Matthew Le, *Junxian He*, Jiatao Gu, Myle Ott, Michael Auli, Marc'Aurelio Ranzato  
EACL 2021. [[arxiv]](https://arxiv.org/abs/1909.13151)

**Dependency Induction Through the Lens of Visual Perception**  
Ruisi Su, Shruti Rijhwani, Hao Zhu, *Junxian He*, Xinyu Wang, Yonatan Bisk, Graham Neubig  
CoNLL 2021. [[arxiv]](https://arxiv.org/abs/2109.09790) [[code]](https://github.com/ruisi-su/concrete_dep)

**Learning Sparse Protoypes for Text Generation**  
*Junxian He*, Taylor Berg-Kirkpatrick, Graham Neubig  
NeurIPS 2020. [[arxiv]](https://arxiv.org/abs/2006.16336) [[code]](https://github.com/jxhe/sparse-text-prototype)

**Revisiting Self-Training for Neural Sequence Generation**  
*Junxian He*\*, Jiatao Gu* (equal contribution), Jiajun Shen, Marc'Aurelio Ranzato  
ICLR 2020. [[arxiv]](https://arxiv.org/abs/1909.13788) [[code]](https://github.com/jxhe/self-training-text-generation)

**A Probabilistic Formulation of Unsupervised Text Style Transfer**  
*Junxian He*\*, Xinyi Wang* (equal contribution), Graham Neubig, Taylor Berg-Kirkpatrick  
ICLR 2020 (<span style="color:red">spotlight, 5%</span>). [[arxiv]](https://arxiv.org/abs/2002.03912) [[code]](https://github.com/cindyxinyiwang/deep-latent-sequence-model)

**On the Sentence Embeddings from Pre-trained Language Models**  
Bohan Li, Hao Zhou, *Junxian He*, Mingxuan Wang, Yiming Yang, Lei Li  
EMNLP 2020. [[arxiv]](https://arxiv.org/abs/2011.05864) [[code]](https://github.com/bohanli/BERT-flow)

**A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text**  
Bohan Li\*, *Junxian He*\* (equal contribution), Graham Neubig, Taylor Berg-Kirkpatrick, Yiming Yang  
EMNLP 2019 (short paper). [[arxiv]](https://arxiv.org/abs/1909.00868) [[code]](https://github.com/bohanli/vae-pretraining-encoder)

**Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections**  
*Junxian He*, Zhisong Zhang, Taylor Berg-Kirkpatrick, Graham Neubig  
ACL 2019. [[arxiv]](https://arxiv.org/abs/1906.02656) [[code]](https://github.com/jxhe/cross-lingual-struct-flow)

**Texar: A modularized, versatile, and extensible toolkit for text generation**  
Zhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang, Zichao Yang, Tiancheng Zhao, *Junxian He*, Lianhui Qin, Di Wang, Xuezhe Ma, Zhengzhong Liu, Xiaodan Liang, Wangrong Zhu, Devendra Singh Sachan, Eric P. Xing  
ACL 2019 (demo paper). <span style="color:red">Best demo paper nomination</span>. [[arxiv]](https://arxiv.org/abs/1809.00794) [[GitHub]](https://github.com/asyml/texar)

**Lagging Inference Networks and Posterior Collapse in Variational Autoencoders**  
*Junxian He*, Daniel Spokoyny, Graham Neubig, Taylor Berg-Kirkpatrick  
ICLR 2019. [[arxiv]](http://arxiv.org/abs/1901.05534) [[code]](https://github.com/jxhe/vae-lagging-encoder)

**Unsupervised Learning of Syntactic Structure with Invertible Neural Projections**   
*Junxian He*, Graham Neubig, Taylor Berg-Kirkpatrick  
EMNLP 2018. [[arxiv]](https://arxiv.org/abs/1808.09111) [[code]](https://github.com/jxhe/struct-learning-with-flow)

**StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing**  
Pengcheng Yin, Chunting Zhou, *Junxian He*, Graham Neubig  
ACL 2018. [[arxiv]](https://arxiv.org/abs/1806.07832)

**Efficient Correlated Topic Modeling with Topic Embedding**  
*Junxian He*\*, Zhiting Hu* (equal contribution), Taylor Berg-Kirkpatrick, Ying Huang, Eric Xing  
KDD 2017. [[arxiv]](https://arxiv.org/abs/1707.00206)

**Text Network Exploration via Heterogeneous Web of Topics**  
*Junxian He*, Ying Huang, Changfeng Liu, Jiaming Shen, Yuting Jia, Xinbing Wang  
ICDM 2016 WorkShop. [[arxiv]](https://arxiv.org/abs/1610.00219) [[demo]]({{ site.baseurl }}/demo/TopicAtlas/CiteseerX.html) 

# Service
Area Chair: EMNLP22, ACL23    
Reviewer: ICLR, NeurIPS, ICML, ACL, EMNLP, NAACL, ARR, TMLR

# Awards
Baidu PhD Fellowship, class of 2020 (10 recipients worldwide)  
Outstanding Undergraduate Thesis in SJTU (top 1%)  
National Scholarship in China (2014/2015/2016)
